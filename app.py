import os, json, re, time, logging, threading
from datetime import datetime, timedelta, date
from collections import deque
from typing import List, Dict, Optional

import pytz
import requests
import feedparser
from bs4 import BeautifulSoup

from flask import Flask, jsonify

# --- Telegram (PTB 13.x) ---
from telegram import Bot, InlineKeyboardButton, InlineKeyboardMarkup, ParseMode

# =============== CONFIG ===============
def env(name, default=None):
    v = os.getenv(name)
    return v if (v is not None and v != "") else default

BOT_TOKEN            = env("BOT_TOKEN")
CHANNEL_ID_RAW       = env("CHANNEL_ID")  # "-100..." or "@yourchannel"
TIMEZONE_NAME        = env("TIMEZONE", "Asia/Kolkata")

ENABLE_NEWS          = env("ENABLE_NEWS", "1") == "1"
NEWS_INTERVAL        = int(env("NEWS_INTERVAL", "30"))        # minutes
MAX_NEWS_PER_SLOT    = int(env("MAX_NEWS_PER_SLOT", "2"))
NEWS_SUMMARY_CHARS   = int(env("NEWS_SUMMARY_CHARS", "550"))

QUIET_HOURS_START    = env("QUIET_HOURS_START", "22:30")       # HH:MM (local)
QUIET_HOURS_END      = env("QUIET_HOURS_END", "07:30")         # HH:MM (local)

ENABLE_IPO           = env("ENABLE_IPO", "1") == "1"
IPO_POST_TIME        = env("IPO_POST_TIME", "10:30")           # HH:MM (Mon‚ÄìFri)

ENABLE_MARKET_BLIPS  = env("ENABLE_MARKET_BLIPS", "1") == "1"  # news window
MARKET_BLIPS_START   = env("MARKET_BLIPS_START", "08:30")      # HH:MM
MARKET_BLIPS_END     = env("MARKET_BLIPS_END", "20:30")        # HH:MM

POSTMARKET_TIME      = env("POSTMARKET_TIME", "20:45")         # HH:MM (Mon‚ÄìFri)

ENABLE_FII_DII       = env("ENABLE_FII_DII", "1") == "1"
FII_DII_POST_TIME    = env("FII_DII_POST_TIME", "21:00")       # HH:MM (Mon‚ÄìFri)

# ====== Safety checks ======
if not BOT_TOKEN:
    raise RuntimeError("BOT_TOKEN is missing in environment!")

# CHANNEL_ID: keep as str (PTB accepts str) ‚Äì works for -100..., numeric, or @handle
CHANNEL_ID = CHANNEL_ID_RAW

TZ = pytz.timezone(TIMEZONE_NAME)

# Logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
)
log = logging.getLogger("marketpulse")

# Telegram bot
bot = Bot(token=BOT_TOKEN)

# Flask (health + keep-alive)
app = Flask(__name__)

# ========== DEDUPE STATE ==========
SEEN_FILE = "/tmp/mpulse_seen.json"
seen_urls = set()
seen_queue = deque(maxlen=1200)  # remember the last ~1200 links

def load_seen():
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        for u in data:
            seen_urls.add(u)
            seen_queue.append(u)
        log.info(f"Loaded {len(seen_urls)} seen URLs")
    except Exception:
        pass

def save_seen():
    try:
        with open(SEEN_FILE, "w", encoding="utf-8") as f:
            json.dump(list(seen_queue), f)
    except Exception:
        pass

load_seen()

# ========== TIME UTILS ==========
def now_local():
    return datetime.now(TZ)

def parse_hhmm(s: str) -> datetime.time:
    h, m = s.split(":")
    return datetime.strptime(f"{int(h):02d}:{int(m):02d}", "%H:%M").time()

def is_weekday(dt: Optional[datetime] = None) -> bool:
    d = dt or now_local()
    return d.weekday() < 5  # Mon=0 .. Sun=6

def within_window(start_str: str, end_str: str, dt: Optional[datetime] = None) -> bool:
    dt = dt or now_local()
    start = parse_hhmm(start_str)
    end   = parse_hhmm(end_str)
    t = dt.time()
    if start <= end:
        return start <= t <= end
    # crosses midnight
    return t >= start or t <= end

def in_quiet_hours(dt: Optional[datetime] = None) -> bool:
    return within_window(QUIET_HOURS_START, QUIET_HOURS_END, dt)

# ========== FETCHERS ==========
# RSS feeds grouped
FEEDS: Dict[str, List[str]] = {
    "market": [
        "https://www.livemint.com/rss/markets",
        "https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms",
        "https://www.business-standard.com/rss/markets-106.rss",
    ],
    "company": [
        "https://www.livemint.com/rss/companies",
        "https://www.moneycontrol.com/rss/latestnews.xml",
        "https://www.business-standard.com/rss/companies-101.rss",
    ],
    "finance": [
        "https://www.livemint.com/rss/economy",
        "https://economictimes.indiatimes.com/industry/rssfeeds/13352306.cms",
        "https://www.business-standard.com/rss/economy-policy-110.rss",
    ],
    "global": [
        "https://feeds.reuters.com/reuters/businessNews",
        "https://feeds.bbci.co.uk/news/business/rss.xml",
        "https://www.aljazeera.com/xml/rss/all.xml",
    ],
}

UA = {
    "User-Agent":
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
}

def clean_text(html_text: str) -> str:
    if not html_text:
        return ""
    soup = BeautifulSoup(html_text, "html.parser")
    txt = soup.get_text(" ", strip=True)
    txt = re.sub(r"\s+", " ", txt)
    return txt

def summarize(text: str, limit: int) -> str:
    if not text:
        return ""
    text = clean_text(text)
    if len(text) <= limit:
        return text
    # cut at nearest sentence end before limit
    cut = text[:limit]
    idx = max(cut.rfind(". "), cut.rfind("! "), cut.rfind("? "))
    if idx >= 0:
        cut = cut[:idx+1]
    return cut

def fetch_feed_entries(url: str) -> List[dict]:
    try:
        feed = feedparser.parse(url)
        out = []
        for e in feed.entries[:15]:
            title = clean_text(e.get("title", ""))
            link  = e.get("link", "")
            desc  = clean_text(e.get("summary", "") or e.get("description", ""))
            out.append({"title": title, "link": link, "summary": desc})
        return out
    except Exception as ex:
        log.warning(f"feed error {url}: {ex}")
        return []

def collect_news_batch(max_items: int) -> List[dict]:
    # Round-robin across groups for diversity
    groups = ["market", "company", "finance", "global"]
    results = []
    for g in groups:
        candidates = []
        for u in FEEDS[g]:
            candidates.extend(fetch_feed_entries(u))
        # filter duplicates & already-seen
        uniq = []
        used = set()
        for c in candidates:
            if not c["link"] or c["link"] in seen_urls or c["link"] in used:
                continue
            used.add(c["link"])
            uniq.append(c)
        results.extend(uniq[:2])  # take top 2 from each group initially
        if len(results) >= max_items:
            break
    # If still short, pool all feeds
    if len(results) < max_items:
        pool = []
        for arr in FEEDS.values():
            for u in arr:
                pool.extend(fetch_feed_entries(u))
        more = []
        used = set(x["link"] for x in results)
        for c in pool:
            if not c["link"] or c["link"] in used or c["link"] in seen_urls:
                continue
            used.add(c["link"])
            more.append(c)
        results.extend(more[: max_items - len(results)])
    return results[:max_items]

# --- IPO (Chittorgarh Ongoing IPOs page) ---
def fetch_ongoing_ipos_for_today() -> List[dict]:
    url = "https://www.chittorgarh.com/ipo/ipo_calendar.asp"
    try:
        r = requests.get(url, headers=UA, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        # Find tables with "Ongoing IPOs" / "Current Mainline IPOs"
        tables = soup.find_all("table")
        rows = []
        for tbl in tables:
            if "IPO" in tbl.get_text(" "):
                for tr in tbl.select("tr"):
                    tds = [clean_text(td.get_text(" ")) for td in tr.find_all("td")]
                    if len(tds) >= 6:
                        rows.append(tds)

        # Heuristic parse
        today = now_local().date()
        today_str = today.strftime("%d %b %Y")  # e.g., 23 Aug 2025

        found = []
        for tds in rows:
            line = " | ".join(tds)
            if any(x in line.lower() for x in ["sme", "bse sme", "nse sme"]):
                pass  # we still allow SME
            # try mapping: [Company, Exchange, Open, Close, ... Price band ... Lot]
            # we only accept if Open <= today <= Close
            try:
                # search dates in row
                m = re.findall(r"(\d{1,2}\s+[A-Za-z]{3}\s+\d{4})", line)
                if len(m) >= 2:
                    open_dt = datetime.strptime(m[0], "%d %b %Y").date()
                    close_dt = datetime.strptime(m[1], "%d %b %Y").date()
                    if open_dt <= today <= close_dt:
                        company = tds[0]
                        # try price band
                        band = None
                        for x in tds:
                            if "‚Çπ" in x or "-" in x:
                                if "‚Çπ" in x and "-" in x:
                                    band = x
                                    break
                        lot = None
                        for x in tds:
                            if "Lot" in x or "Shares" in x:
                                lot = x
                                break
                        found.append({
                            "company": company,
                            "open": open_dt.strftime("%d %b"),
                            "close": close_dt.strftime("%d %b"),
                            "band": band or "",
                            "lot": lot or "",
                            "source": url
                        })
            except Exception:
                continue

        return found
    except Exception as ex:
        log.warning(f"IPO fetch failed: {ex}")
        return []

# --- FII/DII (Moneycontrol page scrape) ---
def fetch_fii_dii_cash() -> Optional[dict]:
    url = "https://www.moneycontrol.com/stocks/marketstats/fii_dii_activity/index.php"
    try:
        r = requests.get(url, headers=UA, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        table = soup.find("table")
        if not table:
            return None

        # Find latest row that has FII, DII cash flow (Rs cr)
        rows = []
        for tr in table.find_all("tr"):
            tds = [clean_text(td.get_text(" ")) for td in tr.find_all("td")]
            if len(tds) >= 5 and re.search(r"\d{1,2}\-\d{1,2}\-\d{4}", tds[0]):
                rows.append(tds)
        if not rows:
            return None

        latest = rows[0]  # date, FII buy/sell/net, DII buy/sell/net ‚Äì page format varies
        # Try different shapes; fallback to numbers with "Net"
        flat = " | ".join(latest)
        m = re.findall(r"Net\s*:?[\s‚Çπ]*([-+]?\d[\d,]*)", flat)
        if len(m) >= 2:
            fii_net = m[0].replace(",", "")
            dii_net = m[1].replace(",", "")
            return {"fii": int(fii_net), "dii": int(dii_net)}
        # coarse: last two numbers in row
        nums = [int(x.replace(",", "")) for x in re.findall(r"[-+]?\d[\d,]*", flat)[-2:]]
        if len(nums) == 2:
            return {"fii": nums[0], "dii": nums[1]}
        return None
    except Exception as ex:
        log.warning(f"FII/DII fetch failed: {ex}")
        return None

# --- Post-market closes via Yahoo Finance ---
def fetch_close_snapshot() -> Optional[dict]:
    # Nifty 50, Sensex, Bank Nifty
    symbols = ["^NSEI", "^BSESN", "^NSEBANK"]
    url = "https://query1.finance.yahoo.com/v7/finance/quote"
    try:
        r = requests.get(url, params={"symbols": ",".join(symbols)}, headers=UA, timeout=12)
        r.raise_for_status()
        data = r.json()["quoteResponse"]["result"]
        def fmt(x):
            if x is None:
                return None
            return round(x, 2)
        mp = {}
        for q in data:
            name = q.get("shortName") or q.get("symbol")
            mp[q["symbol"]] = {
                "name": name,
                "price": fmt(q.get("regularMarketPrice")),
                "change": fmt(q.get("regularMarketChange")),
                "pct": fmt(q.get("regularMarketChangePercent")),
            }
        return mp if mp else None
    except Exception as ex:
        log.warning(f"close snapshot failed: {ex}")
        return None

# ========== SENDER ==========
def send_text(text: str, buttons: Optional[List[List[Dict]]] = None):
    """buttons: [[{'text': 'Read More', 'url': 'http..'}], ...]"""
    markup = None
    if buttons:
        keyboard = [[InlineKeyboardButton(b["text"], url=b["url"]) for b in row] for row in buttons]
        markup = InlineKeyboardMarkup(keyboard)
    bot.send_message(
        chat_id=CHANNEL_ID,
        text=text,
        parse_mode=ParseMode.HTML,
        disable_web_page_preview=True,
        reply_markup=markup
    )

# ========== JOBS ==========
def post_news_slot():
    """Runs every 30 min (7 days). Posts up to MAX_NEWS_PER_SLOT within window."""
    now = now_local()
    if not ENABLE_NEWS:
        return
    if in_quiet_hours(now):
        log.info("news: quiet hours, skip")
        return
    if not within_window(MARKET_BLIPS_START, MARKET_BLIPS_END, now):
        log.info("news: outside window, skip")
        return

    items = collect_news_batch(MAX_NEWS_PER_SLOT)
    if not items:
        log.info("news: nothing new")
        return

    for it in items:
        if it["link"] in seen_urls:
            continue
        title = it["title"] or "Market update"
        summary = summarize(it["summary"], NEWS_SUMMARY_CHARS)
        text = f"<b>{title}</b>\n\n{summary}"
        try:
            send_text(
                text,
                buttons=[[{"text": "Read More", "url": it["link"]}]],
            )
            seen_urls.add(it["link"])
            seen_queue.append(it["link"])
        except Exception as ex:
            log.warning(f"send news failed: {ex}")

    save_seen()

def post_ipo_snapshot():
    """10:30 Mon‚ÄìFri"""
    if not ENABLE_IPO or not is_weekday():
        return
    ipos = fetch_ongoing_ipos_for_today()
    if not ipos:
        text = "üìå <b>IPO</b>\nNo IPO details available today."
        send_text(text)
        return
    # Post one compact block
    lines = ["üìå <b>IPO ‚Äî Ongoing Today</b>"]
    for x in ipos[:6]:
        seg = f"<b>{x['company']}</b> ‚Ä¢ Open {x['open']} ‚Äì Close {x['close']}"
        if x['band']:
            seg += f" ‚Ä¢ {x['band']}"
        if x['lot']:
            seg += f" ‚Ä¢ {x['lot']}"
        lines.append(seg)
    send_text("\n".join(lines))

def post_market_close():
    """20:45 Mon‚ÄìFri"""
    if not ENABLE_MARKET_BLIPS or not is_weekday():
        return
    snap = fetch_close_snapshot()
    if not snap:
        send_text("üìä <b>Post-Market</b>\nMarket closed. (Snapshot unavailable)")
        return

    def ln(sym, label):
        q = snap.get(sym)
        if not q:
            return None
        chg = f"{q['change']:+}" if q['change'] is not None else "‚Äî"
        pct = f"{q['pct']:+}%" if q['pct'] is not None else ""
        return f"{label}: {q['price']} ({chg} | {pct})"

    lines = ["üìä <b>Post-Market ‚Äî Closing Snapshot</b>"]
    for sym, label in [("^BSESN", "Sensex"), ("^NSEI", "Nifty 50"), ("^NSEBANK", "Bank Nifty")]:
        l = ln(sym, label)
        if l:
            lines.append(l)
    send_text("\n".join(lines))

def post_fii_dii():
    """21:00 Mon‚ÄìFri"""
    if not ENABLE_FII_DII or not is_weekday():
        return
    data = fetch_fii_dii_cash()
    if not data:
        send_text("üè¶ <b>FII/DII</b>\nLatest activity not available yet.")
        return
    def fmt(x):
        s = f"{x:+,}"
        return s.replace(",", ",")
    text = (
        "üè¶ <b>FII/DII ‚Äî Cash Market</b>\n"
        f"FII: {fmt(data['fii'])} cr\n"
        f"DII: {fmt(data['dii'])} cr\n"
        "<i>Note: Provisional numbers; subject to revision.</i>"
    )
    send_text(text)

# ========== SCHEDULER ==========
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

scheduler = BackgroundScheduler(
    timezone=TZ,
    job_defaults={"coalesce": True, "max_instances": 1, "misfire_grace_time": 300},
)

def schedule_jobs():
    # every 30 minutes (all days), news window guard is inside the job
    scheduler.add_job(
        post_news_slot,
        trigger=CronTrigger(minute=f"*/{NEWS_INTERVAL}"),
        id="post_news_slot",
        replace_existing=True,
    )

    # Fixed-times (Mon‚ÄìFri only) ‚Äì we don‚Äôt put weekday in trigger,
    # the job itself gates on is_weekday(), so logs still show runs.
    if ENABLE_IPO:
        hh, mm = IPO_POST_TIME.split(":")
        scheduler.add_job(post_ipo_snapshot,
                          trigger=CronTrigger(hour=int(hh), minute=int(mm)),
                          id="post_ipo_snapshot",
                          replace_existing=True)

    if ENABLE_MARKET_BLIPS:
        hh, mm = POSTMARKET_TIME.split(":")
        scheduler.add_job(post_market_close,
                          trigger=CronTrigger(hour=int(hh), minute=int(mm)),
                          id="post_market_close",
                          replace_existing=True)

    if ENABLE_FII_DII:
        hh, mm = FII_DII_POST_TIME.split(":")
        scheduler.add_job(post_fii_dii,
                          trigger=CronTrigger(hour=int(hh), minute=int(mm)),
                          id="post_fii_dii",
                          replace_existing=True)

schedule_jobs()
scheduler.start()

# Announce on startup
def announce_startup():
    start = MARKETING_WINDOW = f"{MARKET_BLIPS_START}‚Äì{MARKET_BLIPS_END}"
    msg = (
        "‚úÖ <b>MarketPulse bot restarted and schedule loaded.</b>\n"
        f"Window: {MARKET_BLIPS_START}‚Äì{MARKET_BLIPS_END} ‚Ä¢ Every {NEWS_INTERVAL} min ‚Ä¢ Max {MAX_NEWS_PER_SLOT}/slot\n"
        f"Quiet: {QUIET_HOURS_START}‚Äì{QUIET_HOURS_END}\n"
        f"Fixed posts: {IPO_POST_TIME} IPO ‚Ä¢ {POSTMARKET_TIME} Post-market ‚Ä¢ {FII_DII_POST_TIME} FII/DII\n"
        "<i>News runs daily; fixed posts Mon‚ÄìFri.</i>"
    )
    try:
        send_text(msg)
    except Exception as ex:
        log.warning(f"startup announce failed: {ex}")

threading.Thread(target=announce_startup, daemon=True).start()

# ========== FLASK ==========
@app.route("/", methods=["GET", "HEAD"])
def root():
    jobs = []
    for j in scheduler.get_jobs():
        jobs.append({
            "id": j.id,
            "next_run": j.next_run_time.astimezone(TZ).strftime("%Y-%m-%d %H:%M:%S") if j.next_run_time else None
        })
    return jsonify({
        "ok": True,
        "tz": TIMEZONE_NAME,
        "now": now_local().strftime("%Y-%m-%d %H:%M:%S"),
        "jobs": jobs,
        "news_window": [MARKET_BLIPS_START, MARKET_BLIPS_END],
        "quiet": [QUIET_HOURS_START, QUIET_HOURS_END],
        "seen": len(seen_urls)
    })

# For gunicorn
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "10000")))
