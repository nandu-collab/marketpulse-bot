# app.py
# MarketPulse ‚Äî combined news + scheduled market posts with retries and holiday check
# Designed to run under Gunicorn: `gunicorn app:app --workers 1 --timeout 180`

import os
import json
import re
import time
import logging
import threading
from datetime import datetime, timedelta, date
from collections import deque
from typing import List, Dict, Optional
import sys

# Temporary patch for feedparser on Python 3.13 (cgi removed)
import feedparser
if not hasattr(feedparser.encodings, "convert_to_utf8"):
    def _noop(data, declared_encoding=None, is_html=False):
        return data, declared_encoding
    feedparser.encodings.convert_to_utf8 = _noop
    
import pytz
import requests
import feedparser
from bs4 import BeautifulSoup

from flask import Flask, jsonify
from telegram import Bot, InlineKeyboardButton, InlineKeyboardMarkup, ParseMode

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

# ------------------------ Configuration helpers ------------------------
def env(name: str, default=None):
    v = os.getenv(name)
    return v if (v is not None and v != "") else default

BOT_TOKEN         = env("BOT_TOKEN")
CHANNEL_ID        = env("CHANNEL_ID")        # must be -100... for channels
TIMEZONE_NAME     = env("TIMEZONE", "Asia/Kolkata")

NEWS_SUMMARY_CHARS = int(env("NEWS_SUMMARY_CHARS", "550"))
MAX_NEWS_PER_SLOT  = int(env("MAX_NEWS_PER_SLOT", "2"))

# market windows / times (defaults you requested)
MARKET_BLIPS_START = env("MARKET_BLIPS_START", "08:30")
MARKET_BLIPS_END   = env("MARKET_BLIPS_END", "21:30")

PREMARKET_TIME     = env("PREMARKET_TIME", "09:00")
IPO_POST_TIME      = env("IPO_POST_TIME", "11:00")
POSTMARKET_TIME    = env("POSTMARKET_TIME", "16:00")
FII_DII_POST_TIME  = env("FII_DII_POST_TIME", "21:00")

# Poll / retry settings for fixed posts (try repeatedly for this window)
POLL_RETRY_INTERVAL_SEC = int(env("POLL_RETRY_INTERVAL_SEC", "600"))  # 10 minutes
POLL_WINDOW_MIN = int(env("POLL_WINDOW_MIN", "120"))  # try for up to 2 hours after target time

# Keepalive / self-ping interval (minutes)
SELF_PING_INTERVAL_MIN = int(env("SELF_PING_INTERVAL_MIN", "10"))

# safety checks
if not BOT_TOKEN:
    raise RuntimeError("BOT_TOKEN is required")
if not CHANNEL_ID:
    raise RuntimeError("CHANNEL_ID is required")

# timezone object (pytz so APScheduler works correctly)
TZ = pytz.timezone(TIMEZONE_NAME)

# ------------------------ Logging ------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("marketpulse")

# ------------------------ Telegram bot ------------------------
bot = Bot(token=BOT_TOKEN)

# ------------------------ Flask (health + ping) ------------------------
app = Flask(__name__)

# ------------------------ Dedupe state ------------------------
SEEN_FILE = "/tmp/mpulse_seen.json"
seen_urls = set()
seen_queue = deque(maxlen=2000)

def load_seen():
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            arr = json.load(f)
        for u in arr:
            seen_urls.add(u)
            seen_queue.append(u)
        log.info("Loaded %d seen URLs", len(seen_urls))
    except Exception:
        log.info("No seen file or failed to load")

def save_seen():
    try:
        with open(SEEN_FILE, "w", encoding="utf-8") as f:
            json.dump(list(seen_queue), f)
    except Exception as e:
        log.warning("save_seen failed: %s", e)

load_seen()

# ------------------------ Utilities ------------------------
UA_POOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
    "Mozilla/5.0 (compatible; MarketPulseBot/1.0; +https://example.com/bot)",
]
def ua():
    return {"User-Agent": UA_POOL[int(time.time()) % len(UA_POOL)]}

def now_local():
    return datetime.now(TZ)

def parse_hhmm(s: str):
    h, m = s.split(":")
    return int(h), int(m)

def within_window(start_str: str, end_str: str, dt: Optional[datetime] = None) -> bool:
    dt = dt or now_local()
    sh, sm = parse_hhmm(start_str); eh, em = parse_hhmm(end_str)
    start = dt.replace(hour=sh, minute=sm, second=0, microsecond=0).time()
    end   = dt.replace(hour=eh, minute=em, second=0, microsecond=0).time()
    t = dt.time()
    if start <= end:
        return start <= t <= end
    # crosses midnight
    return t >= start or t <= end

def in_quiet_hours(dt: Optional[datetime] = None) -> bool:
    # quiet hours around night (if you need)
    return False  # keep simple; modify if you want.

def clean_text(html_text: str) -> str:
    if not html_text:
        return ""
    soup = BeautifulSoup(html_text, "html.parser")
    txt = soup.get_text(" ", strip=True)
    return re.sub(r"\s+", " ", txt)

def summarize(text: str, limit: int) -> str:
    if not text:
        return ""
    t = clean_text(text)
    if len(t) <= limit:
        return t
    cut = t[:limit]
    idx = max(cut.rfind(". "), cut.rfind("? "), cut.rfind("! "))
    if idx > 0:
        return cut[:idx+1]
    return cut.rstrip() + "‚Ä¶"

# ------------------------ Feeds + news collection ------------------------
FEEDS: Dict[str, List[str]] = {
    "market": [
        "https://www.livemint.com/rss/markets",
        "https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms",
        "https://www.business-standard.com/rss/markets-106.rss",
    ],
    "company": [
        "https://www.livemint.com/rss/companies",
        "https://www.moneycontrol.com/rss/latestnews.xml",
    ],
    "finance": [
        "https://www.livemint.com/rss/economy",
        "https://economictimes.indiatimes.com/industry/rssfeeds/13352306.cms",
    ],
    "global": [
        "https://feeds.reuters.com/reuters/businessNews",
        "https://feeds.bbci.co.uk/news/business/rss.xml",
    ],
}

def fetch_feed_entries(url: str, limit=12) -> List[dict]:
    try:
        feed = feedparser.parse(url)
        out = []
        for e in feed.entries[:limit]:
            title = clean_text(e.get("title", ""))
            link  = e.get("link", "")
            desc  = clean_text(e.get("summary", "") or e.get("description", ""))
            if title and link:
                out.append({"title": title, "link": link, "summary": desc})
        return out
    except Exception as ex:
        log.warning("feed error %s: %s", url, ex)
        return []

def collect_news_batch(max_items: int) -> List[dict]:
    groups = ["market", "company", "finance", "global"]
    results = []
    for g in groups:
        candidates = []
        for u in FEEDS[g]:
            candidates.extend(fetch_feed_entries(u))
        uniq = []
        used = set()
        for c in candidates:
            if not c["link"] or c["link"] in used or c["link"] in seen_urls:
                continue
            used.add(c["link"])
            uniq.append(c)
        results.extend(uniq[:2])
        if len(results) >= max_items:
            break
    if len(results) < max_items:
        pool = []
        for arr in FEEDS.values():
            for u in arr:
                pool.extend(fetch_feed_entries(u))
        more = []
        used = set(x["link"] for x in results)
        for c in pool:
            if not c["link"] or c["link"] in used or c["link"] in seen_urls:
                continue
            used.add(c["link"])
            more.append(c)
        results.extend(more[: max_items - len(results)])
    return results[:max_items]

# ------------------------ Special fetchers ------------------------
# IPO fetch (chittorgarh) - best effort for mainboard IPOs
def fetch_ongoing_ipos_for_today() -> List[dict]:
    url = "https://www.chittorgarh.com/ipo/ipo_calendar.asp"
    try:
        r = requests.get(url, headers=ua(), timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        tables = soup.find_all("table")
        rows = []
        for tbl in tables:
            if "IPO" in tbl.get_text(" "):
                for tr in tbl.select("tr"):
                    tds = [clean_text(td.get_text(" ")) for td in tr.find_all("td")]
                    if len(tds) >= 5:
                        rows.append(tds)
        found = []
        today = now_local().date()
        from datetime import datetime as _dt
        for tds in rows:
            line = " | ".join(tds)
            m = re.findall(r"(\d{1,2}\s+[A-Za-z]{3}\s+\d{4})", line)
            if len(m) >= 2:
                try:
                    op = _dt.strptime(m[0], "%d %b %Y").date()
                    cl = _dt.strptime(m[1], "%d %b %Y").date()
                    if op <= today <= cl:
                        company = tds[0]
                        band = ""
                        for x in tds:
                            if "‚Çπ" in x and "-" in x:
                                band = x; break
                        lot = ""
                        for x in tds:
                            if "Lot" in x or "Shares" in x:
                                lot = x; break
                        found.append({"company": company, "open": op.strftime("%d %b"), "close": cl.strftime("%d %b"), "band": band, "lot": lot})
                except Exception:
                    continue
        return found
    except Exception as ex:
        log.warning("IPO fetch failed: %s", ex)
        return []

# FII/DII fetch ‚Äî best-effort from MoneyControl; returns {"fii": int, "dii": int} or None
def fetch_fii_dii_cash() -> Optional[dict]:
    url = "https://www.moneycontrol.com/stocks/marketstats/fii_dii_activity/index.php"
    try:
        r = requests.get(url, headers=ua(), timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        table = soup.find("table")
        if not table:
            return None
        rows = []
        for tr in table.find_all("tr"):
            tds = [clean_text(td.get_text(" ")) for td in tr.find_all("td")]
            if len(tds) >= 4 and re.search(r"\d{1,2}\-\d{1,2}\-\d{4}", " ".join(tds)):
                rows.append(tds)
        if not rows:
            return None
        latest = rows[0]
        flat = " | ".join(latest)
        m = re.findall(r"Net\s*:?[\s‚Çπ]*([-+]?\d[\d,]*)", flat)
        if len(m) >= 2:
            try:
                fii = int(m[0].replace(",", ""))
                dii = int(m[1].replace(",", ""))
                return {"fii": fii, "dii": dii}
            except Exception:
                pass
        nums = re.findall(r"[-+]?\d[\d,]*", flat)
        if len(nums) >= 2:
            try:
                return {"fii": int(nums[-2].replace(",", "")), "dii": int(nums[-1].replace(",", ""))}
            except Exception:
                pass
        return None
    except Exception as ex:
        log.warning("FII/DII fetch failed: %s", ex)
        return None

# Post-market snapshot via Yahoo Quote API (best-effort)
def fetch_close_snapshot() -> Optional[dict]:
    symbols = ["^NSEI", "^BSESN", "^NSEBANK"]
    url = "https://query1.finance.yahoo.com/v7/finance/quote"
    try:
        r = requests.get(url, params={"symbols": ",".join(symbols)}, headers=ua(), timeout=12)
        r.raise_for_status()
        data = r.json().get("quoteResponse", {}).get("result", [])
        if not data:
            return None
        def fmt(x):
            return round(x, 2) if (isinstance(x, (int, float))) else None
        mp = {}
        for q in data:
            sym = q.get("symbol")
            mp[sym] = {"price": fmt(q.get("regularMarketPrice")),
                       "change": fmt(q.get("regularMarketChange")),
                       "pct": fmt(q.get("regularMarketChangePercent"))}
        return mp or None
    except Exception as ex:
        log.warning("close snapshot failed: %s", ex)
        return None

# Optionally fetch SGX/FT/futures data for premarket (best-effort)
def fetch_sgx_nifty() -> Optional[dict]:
    # best-effort: try Yahoo or other endpoints; if fails return None
    url = "https://query1.finance.yahoo.com/v7/finance/quote"
    try:
        # SGX Nifty often symbol is "^NSEI" for India; SGX symbol may differ and be hard to standardize.
        r = requests.get(url, params={"symbols": "%5ENSEI"}, headers=ua(), timeout=10)
        r.raise_for_status()
        data = r.json().get("quoteResponse", {}).get("result", [])
        if data:
            q = data[0]
            return {"price": q.get("regularMarketPrice"), "change": q.get("regularMarketChange"), "pct": q.get("regularMarketChangePercent")}
    except Exception:
        pass
    return None

# ------------------------ Sending helper ------------------------
def send_text(text: str, button_url: Optional[str] = None, button_text: str = "Read more"):
    try:
        markup = None
        if button_url:
            kb = [[InlineKeyboardButton(button_text, url=button_url)]]
            markup = InlineKeyboardMarkup(kb)
        bot.send_message(chat_id=CHANNEL_ID, text=text, parse_mode=ParseMode.HTML, disable_web_page_preview=True, reply_markup=markup)
    except Exception as ex:
        log.warning("Telegram send failed: %s", ex)

# ------------------------ Scheduler jobs ------------------------
def post_news_slot():
    now = now_local()
    # news runs 7 days a week (user requested)
    if not within_window(MARKET_BLIPS_START, MARKET_BLIPS_END, now):
        log.info("news: outside window -> skip")
        return
    items = collect_news_batch(MAX_NEWS_PER_SLOT)
    if not items:
        log.info("news: nothing to post")
        return
    posted = 0
    for it in items:
        if it["link"] in seen_urls:
            continue
        title = it["title"] or "Market update"
        summary = summarize(it["summary"], NEWS_SUMMARY_CHARS)
        text = f"<b>{title}</b>\n\n{summary}"
        send_text(text, button_url=it["link"], button_text="Read more ‚Üí")
        seen_urls.add(it["link"])
        seen_queue.append(it["link"])
        posted += 1
        time.sleep(1)
    if posted:
        save_seen()
    log.info("news_slot posted %d items", posted)

# Helper: is trading day? weekday + not in holiday set
HOLIDAYS = set()
HOLIDAYS_LAST_REFRESH = None

def fetch_nse_holidays() -> set:
    """
    Try fetch NSE holiday JSON; if fails, attempt a raw GitHub fallback that you can update;
    final fallback: empty set (no holidays).
    Returns set of date strings YYYY-MM-DD.
    """
    global HOLIDAYS_LAST_REFRESH
    try:
        # Try official NSE API (best-effort)
        # Note: endpoint details may change; many regions block programmatic requests.
        urls = [
            "https://www.nseindia.com/api/holiday-master?type=trading",  # possible endpoint (may require headers)
            # fallback raw file in your GitHub (you can provide your own)
            "https://raw.githubusercontent.com/nandan-collab/marketpulse-data/main/nse-holidays-2025.json"
        ]
        out = set()
        for u in urls:
            try:
                r = requests.get(u, headers=ua(), timeout=10)
                if r.status_code != 200:
                    continue
                j = r.json()
                # official NSE: may have 'data' with keys for 'trading' etc; try to find dates
                if isinstance(j, dict):
                    # try multiple structures
                    if "holidayDates" in j:
                        for d in j["holidayDates"]:
                            out.add(d.get("date"))
                    elif "data" in j and isinstance(j["data"], list):
                        # entries may have 'date'
                        for item in j["data"]:
                            if isinstance(item, dict) and "date" in item:
                                out.add(item["date"])
                            elif isinstance(item, str) and re.match(r"\d{4}-\d{2}-\d{2}", item):
                                out.add(item)
                    else:
                        # try scanning for yyyy-mm-dd strings anywhere
                        dates = re.findall(r"\d{4}-\d{2}-\d{2}", json.dumps(j))
                        for d in dates:
                            out.add(d)
                # if it succeeded return set
                if out:
                    HOLIDAYS_LAST_REFRESH = datetime.utcnow()
                    return out
            except Exception:
                continue
        # final fallback: empty (we will behave as "no holidays found")
        HOLIDAYS_LAST_REFRESH = datetime.utcnow()
        return set()
    except Exception as ex:
        log.warning("holiday fetch failed: %s", ex)
        return set()

def is_trading_day(dt: Optional[datetime] = None) -> bool:
    dt = dt or now_local()
    if dt.weekday() >= 5:
        return False
    # refresh holiday list once per day
    global HOLIDAYS, HOLIDAYS_LAST_REFRESH
    if HOLIDAYS_LAST_REFRESH is None or (datetime.utcnow() - HOLIDAYS_LAST_REFRESH).total_seconds() > 24*3600:
        try:
            HOLIDAYS = fetch_nse_holidays()
            log.info("Holidays refreshed (%d)", len(HOLIDAYS))
        except Exception as e:
            log.warning("Failed to refresh holidays: %s", e)
    ymd = dt.strftime("%Y-%m-%d")
    return ymd not in HOLIDAYS

# Common pattern: do repeated polls for up to POLL_WINDOW_MIN minutes
def attempt_with_polling(target_fn, window_minutes=POLL_WINDOW_MIN, interval_seconds=POLL_RETRY_INTERVAL_SEC):
    """
    Call target_fn() repeatedly until it returns truthy data or window expires.
    Returns the data from target_fn or None.
    """
    start = now_local()
    deadline = start + timedelta(minutes=window_minutes)
    attempt = 0
    while now_local() <= deadline:
        attempt += 1
        try:
            res = target_fn()
            if res:
                log.info("attempt_with_polling: success on attempt %d", attempt)
                return res
        except Exception as ex:
            log.warning("attempt_with_polling: attempt %d failed: %s", attempt, ex)
        time.sleep(interval_seconds)
    log.info("attempt_with_polling: window expired after %d attempts", attempt)
    return None

def post_pre_market():
    # run only on trading days
    if not is_trading_day():
        log.info("pre-market: not a trading day -> skip")
        return
    # We'll try to get a concise premarket snapshot: SGX (if available) + top cues
    def _fetch():
        # try SGX / futures via fetch_sgx_nifty()
        sgx = fetch_sgx_nifty()
        # collect 4 latest headlines for context
        headlines = collect_news_batch(4)
        bullets = []
        if sgx and sgx.get("price") is not None:
            bullets.append(f"Gift Nifty: {sgx['price']} ({sgx.get('pct'):+}%)")
        for h in headlines[:4]:
            bullets.append(h["title"])
        if bullets:
            return {"bullets": bullets}
        return None

    data = attempt_with_polling(_fetch)
    if not data:
        log.info("pre-market: no reliable data found -> skip")
        return
    text = "üìà <b>[Pre-Market Brief]</b>\n\nKey overnight / early cues:\n"
    text += "\n".join([f"‚Ä¢ {b}" for b in data["bullets"]])
    send_text(text)
    log.info("posted pre-market")

def post_ipo_snapshot():
    if not is_trading_day():
        log.info("IPO: not a trading day -> skip")
        return
    def _fetch():
        ipos = fetch_ongoing_ipos_for_today()
        return ipos if ipos else None
    data = attempt_with_polling(_fetch)
    if not data:
        log.info("IPO: nothing found -> skip")
        return
    lines = ["üìå <b>IPO ‚Äî Ongoing Today</b>"]
    for x in data[:6]:
        seg = f"<b>{x['company']}</b> ‚Ä¢ Open {x['open']} ‚Äì Close {x['close']}"
        if x.get("band"):
            seg += f" ‚Ä¢ {x['band']}"
        if x.get("lot"):
            seg += f" ‚Ä¢ {x['lot']}"
        lines.append(seg)
    send_text("\n".join(lines))
    log.info("posted IPO snapshot")

def post_post_market():
    if not is_trading_day():
        log.info("post-market: not a trading day -> skip")
        return
        data = attempt_with_polling(fetch_close_snapshot)
    if not data:
        log.info("post-market: snapshot unavailable -> skip")
        return
    parts = ["üìä <b>Post-Market ‚Äî Closing Snapshot</b>"]
    for sym, label in [("^BSESN", "Sensex"), ("^NSEI", "Nifty 50"), ("^NSEBANK", "Bank Nifty")]:
        q = data.get(sym)
        if q and q.get("price") is not None:
            chg = f"{q['change']:+}" if q['change'] is not None else "‚Äî"
            pct = f"{q['pct']:+}%" if q['pct'] is not None else ""
            parts.append(f"{label}: {q['price']} ({chg} | {pct})")
    send_text("\n".join(parts))
    log.info("posted post-market")

def post_fii_dii():
    if not is_trading_day():
        log.info("FII/DII: not a trading day -> skip")
        return
    data = attempt_with_polling(fetch_fii_dii_cash)
    if not data:
        log.info("FII/DII: data not found -> skip")
        return
    def fmt(n):
        sign = "+" if n >= 0 else ""
        return f"{sign}{n:,}"
    text = f"üè¶ <b>FII/DII ‚Äî Cash</b>\nFII: {fmt(data['fii'])} cr\nDII: {fmt(data['dii'])} cr\n<i>Provisional</i>"
    send_text(text)
    log.info("posted FII/DII")

# ------------------------ Scheduler setup ------------------------
sched = BackgroundScheduler(timezone=TZ, job_defaults={"coalesce": True, "max_instances": 1, "misfire_grace_time": 300})

def schedule_jobs():
    # News: every day at :30 between hours (MARKET_BLIPS_START..MARKET_BLIPS_END)
    sh, sm = parse_hhmm(MARKET_BLIPS_START)
    eh, em = parse_hhmm(MARKET_BLIPS_END)
    # We'll schedule cron across hours 8-21 minute 30 by default; compute hours range
    # Simplify: run each hour between the hours values inclusive
    hstart = sh
    hend = eh
    cron_hours = f"{hstart}-{hend}"
    sched.add_job(post_news_slot, trigger=CronTrigger(hour=cron_hours, minute=30, timezone=TZ), id="post_news_slot", replace_existing=True)

    # Pre-market
    hh, mm = parse_hhmm(PREMARKET_TIME)
    sched.add_job(post_pre_market, trigger=CronTrigger(hour=hh, minute=mm, timezone=TZ), id="pre_market", replace_existing=True)

    # IPO
    hh, mm = parse_hhmm(IPO_POST_TIME)
    sched.add_job(post_ipo_snapshot, trigger=CronTrigger(hour=hh, minute=mm, timezone=TZ), id="ipo_snapshot", replace_existing=True)

    # Post-market
    hh, mm = parse_hhmm(POSTMARKET_TIME)
    sched.add_job(post_post_market, trigger=CronTrigger(hour=hh, minute=mm, timezone=TZ), id="post_market", replace_existing=True)

    # FII/DII
    hh, mm = parse_hhmm(FII_DII_POST_TIME)
    sched.add_job(post_fii_dii, trigger=CronTrigger(hour=hh, minute=mm, timezone=TZ), id="fii_dii", replace_existing=True)

schedule_jobs()
sched.start()
log.info("Scheduler started with jobs:")
for j in sched.get_jobs():
    try:
        nxt = j.next_run_time.astimezone(TZ).strftime("%Y-%m-%d %H:%M:%S") if j.next_run_time else None
    except Exception:
        nxt = str(j.next_run_time)
    log.info(" - %s next=%s", j.id, nxt)
  # ------------------------ Keepalive / self-ping ------------------------
# Add a simple background job that hits /ping to keep the service warm and help avoid dependence on UptimeRobot.
def self_ping():
    try:
        url = env("SELF_PING_URL") or None
        if url:
            # If user provided a custom monitor URL, ping it
            requests.get(url, timeout=6)
        else:
            # ping local root
            base = env("SERVICE_URL") or None
            if base:
                try:
                    requests.get(base + "/ping", timeout=6)
                except Exception:
                    # try root if ping fails
                    requests.get(base, timeout=6)
            else:
                # Nothing to ping (Render doesn't allow calling 127.0.0.1 externally)
                pass
        log.debug("self_ping OK")
    except Exception as ex:
        log.debug("self_ping failed: %s", ex)

def schedule_self_ping():
    # run every SELF_PING_INTERVAL_MIN minutes in background thread
    def run():
        while True:
            self_ping()
            time.sleep(SELF_PING_INTERVAL_MIN * 60)
    t = threading.Thread(target=run, daemon=True)
    t.start()

schedule_self_ping()

# Announce start (best-effort)
def announce_startup():
    try:
        text = (
            "‚úÖ <b>MarketPulse started</b>\n"
            f"News: every hour at :30 between {MARKET_BLIPS_START}‚Äì{MARKET_BLIPS_END}\n"
            f"Pre-market: {PREMARKET_TIME} ‚Ä¢ IPO: {IPO_POST_TIME} ‚Ä¢ Post-market: {POSTMARKET_TIME} ‚Ä¢ FII/DII: {FII_DII_POST_TIME}\n"
            "<i>Market posts only on trading days (Mon‚ÄìFri + NSE holiday check).</i>"
        )
        send_text(text)
    except Exception as ex:
        log.warning("announce failed: %s", ex)

threading.Thread(target=announce_startup, daemon=True).start()
# ------------------------ Flask endpoints ------------------------
@app.route("/", methods=["GET", "HEAD"])
def root():
    return "MarketPulse running", 200

@app.route("/ping", methods=["GET"])
def ping():
    return "pong", 200

@app.route("/status", methods=["GET"])
def status():
    jobs = []
    for j in sched.get_jobs():
        try:
            nxt = j.next_run_time.astimezone(TZ).strftime("%Y-%m-%d %H:%M:%S") if j.next_run_time else None
        except Exception:
            nxt = str(j.next_run_time)
        jobs.append({"id": j.id, "next_run": nxt})
    return jsonify({"ok": True, "tz": TIMEZONE_NAME, "now": now_local().strftime("%Y-%m-%d %H:%M:%S"), "jobs": jobs, "seen": len(seen_urls)})

# Local run helper
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "10000")))
